{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import math, statistics\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to map the 2011 district_ids to their corresponding 2001 district_ids.\n",
    "file = open('district_mapping.csv')\n",
    "mapping = file.read()[3:].split('\\n')[:-1]\n",
    "file.close()\n",
    "\n",
    "# Forming a dictionary to map the 2011 district_ids to their corresponding 2001 district_ids.\n",
    "temp = {}\n",
    "for i in mapping:\n",
    "    i = i.split(',')\n",
    "    temp[int(i[0])] = int(i[1])\n",
    "mapping = temp\n",
    "\n",
    "# Load the data to map the 2011 labels of districts.\n",
    "file = open('labels_2011.csv')\n",
    "labels = file.read()[3:].split('\\n')[:-1]\n",
    "file.close()\n",
    "\n",
    "# Forming a dictionary to map the 2011 district_ids to their corresponding labels.\n",
    "temp = {}\n",
    "for i in labels:\n",
    "    i = i.split(',')\n",
    "    temp[int(i[0])] = i[1]\n",
    "labels = temp\n",
    "\n",
    "# Load the data to map the 2019 change predictions of districts.\n",
    "file = open('change_predictions_2019.csv')\n",
    "pace = file.read().split('\\n')[1:-1]\n",
    "file.close()\n",
    "\n",
    "# Forming a dictionary to map the 2011 district_ids to their predicted pace of growth.\n",
    "temp = {}\n",
    "for i in pace:\n",
    "    i = i.split(',')\n",
    "    if int(mapping[int(i[0])]) in temp:\n",
    "        temp[int(mapping[int(i[0])])].append(int(i[1]))\n",
    "    else:\n",
    "        temp[int(mapping[int(i[0])])] = [int(i[1])]\n",
    "for i in temp:\n",
    "    temp[i] = math.ceil(statistics.mean(temp[i]))\n",
    "    if temp[i]<2:\n",
    "        temp[i]='Slow'\n",
    "    elif temp[i]==2:\n",
    "        temp[i]='Average'\n",
    "    else:\n",
    "        temp[i]='Fast'\n",
    "pace = temp\n",
    "\n",
    "# Load the data to map the 2011 labels of districts.\n",
    "file = open('industry_type.csv')\n",
    "industry = file.read().split('\\n')[1:-1]\n",
    "file.close()\n",
    "\n",
    "# Forming a dictionary to map the 2011 district_ids to their industrial type.\n",
    "temp={}\n",
    "for i in industry:\n",
    "    i = i.split(',')\n",
    "    temp[int(i[0])] = 'Type-'+i[1]\n",
    "industry = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all the entities, for entities blinding.\n",
    "file = open('locations.csv')\n",
    "data = file.read().split('\\n')[1:]\n",
    "file.close()\n",
    "\n",
    "locations = set()\n",
    "for i in data:\n",
    "    i = i.split(',')[1].split()\n",
    "    locations |= set(i)\n",
    "    \n",
    "# function to remove stopwords, perform stemming and entity blinding.\n",
    "def filter_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text)\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    text = [w for w in text if not w in stop_words|locations]\n",
    "    text = [ps.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create the required data.\n",
    "def create_dataset(collection_name):\n",
    "    \n",
    "    print(collection_name.capitalize())\n",
    "    path = './Collections/'+collection_name.capitalize()+' Data/'\n",
    "    \n",
    "    file1 = open(path + 'file1','rb')\n",
    "    file2 = open(path + 'file2','rb')\n",
    "    data = pickle.loads(zlib.decompress(pickle.load(file1)))\n",
    "    text = pickle.loads(zlib.decompress(pickle.load(file2)))\n",
    "    file1.close()\n",
    "    file2.close()\n",
    "    \n",
    "    temp = {}\n",
    "    for i in text:\n",
    "        temp[i[0]] = [i[1], i[2], i[3]]\n",
    "    text = temp\n",
    "        \n",
    "    district_data = []\n",
    "    states_data = []\n",
    "    processed = 0\n",
    "    \n",
    "    for i in data:\n",
    "        \n",
    "        i[1] = int(i[1])\n",
    "        if i[1]<900 and i[0] in text:\n",
    "            i[1] = mapping[i[1]]\n",
    "            district_data.append([i[0], text[i[0]][0], text[i[0]][1], filter_text(text[i[0]][0]+' '+text[i[0]][1]), \\\n",
    "             text[i[0]][2], mapping[i[1]], labels[i[1]], pace[i[1]], industry[i[1]]])\n",
    "        elif i[1]>=900 and i[0] in text:\n",
    "            states_data.append([i[0], text[i[0]][0], text[i[0]][1], filter_text(text[i[0]][0]+' '+text[i[0]][1]), \\\n",
    "             text[i[0]][2]])   \n",
    "        \n",
    "        processed+=1\n",
    "        print(collection_name.capitalize(), processed,'/',len(data))\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    print('Saving Datasets.')\n",
    "    \n",
    "    file = open('./DT2V_Datasets/dataset_'+collection_name,'wb')\n",
    "    pickle.dump(zlib.compress(pickle.dumps(district_data),pickle.HIGHEST_PROTOCOL),file,pickle.HIGHEST_PROTOCOL)\n",
    "    file.close()\n",
    "    \n",
    "    file = open('./DT2V_Datasets/states_dataset_'+collection_name,'wb')\n",
    "    pickle.dump(zlib.compress(pickle.dumps(states_data),pickle.HIGHEST_PROTOCOL),file,pickle.HIGHEST_PROTOCOL)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Datasets.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# os.mkdir('./DT2V_Dataset/')\n",
    "for collection in ['agriculture','development','environment','industrialization','environment']:\n",
    "    create_dataset(collection)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
