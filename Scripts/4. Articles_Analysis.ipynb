{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import numpy as np\n",
    "import pickle, zlib\n",
    "from random import sample\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TF-IDF keywords and scores.\n",
    "def display_scores(vectorizer, tfidf_result):\n",
    "    scores = zip(vectorizer.get_feature_names(),np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    tf_idf_words, tf_idf_scores = [], []\n",
    "    useless_words = set(['offici','said','govern','near','irregular','special','say','ad','minist','chief','clash','bodi','local','time','work','vigil','mla','region', 'get','start','member','mahatma','congress','state','gram','depart', 'rs', 'crore', 'also', 'card', 'district', 'tuesday', 'offic', 'year', 'meet', 'day', 'would', 'peopl', 'nation', 'lakh', 'plan', 'union', 'alleg', 'provid', 'two', 'km', 'taken', 'guarante', 'take', 'complet', 'report', 'case', 'found', 'per', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'monday', 'tuesday', 'wednesday', 'thrusday', 'friday', 'saturday', 'sunday', 'issu', 'ask', 'level', 'order', 'parti', 'director', 'releas', 'bjp', 'sourc', 'cm', 'injur', 'mr', 'ramesh', 'visit', 'act', 'famili', 'secretari', 'first', 'last', 'includ', 'probe', 'direct', 'month'])\n",
    "    for item in sorted_scores:\n",
    "        if item[0] not in useless_words:\n",
    "            tf_idf_words.append(item[0])\n",
    "            tf_idf_scores.append(np.round(item[1],2))\n",
    "    return tf_idf_words,tf_idf_scores\n",
    "\n",
    "# Resolve article_ids to their corresponding titles and tf_idf_keywords.\n",
    "def resolve_articles(ids, dataset):\n",
    "    data = {}\n",
    "    for i in dataset:\n",
    "        if i[0] not in data:\n",
    "            data[i[0]] = [i[1],i[3]]\n",
    "    titles = []\n",
    "    resolved_text = []\n",
    "    for i in ids:\n",
    "        titles.append(data[i][0])\n",
    "        resolved_text.append(data[i][1])\n",
    "    for i in range(len(resolved_text)):\n",
    "        temp = ''\n",
    "        for j in resolved_text[i]:\n",
    "            temp+=(j+' ')\n",
    "        resolved_text[i] = temp      \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_result = vectorizer.fit_transform(resolved_text)\n",
    "    result = display_scores(vectorizer, tfidf_result)\n",
    "    return titles, result[0], result[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Hierarchial Clustering.\n",
    "def hierarchial_clustering(data1, data2, method='cosine'):\n",
    "\n",
    "    # If number of articles are greater than 30k, then randomly sample 30k.\n",
    "    limit = 30000\n",
    "    if len(data1)>limit:\n",
    "        data2 = sample(data2,limit)\n",
    "        data1 = [i[1] for i in data2]\n",
    "\n",
    "    linkage_matrix = sch.linkage(data1, 'complete', metric=method)\n",
    "    \n",
    "    for threshold in np.arange(1.5,0,-0.05):\n",
    "        labels = sch.fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "        n_clusters = len(set(labels))\n",
    "        if n_clusters>=50 and n_clusters<=100:\n",
    "            break\n",
    "    \n",
    "    print('Number of articles:',len(data1),'\\tNumber of clusters:',n_clusters)\n",
    "\n",
    "    global_centroid = np.median(data1,axis=0)\n",
    "    \n",
    "    data = np.hstack((np.array(data2),labels.reshape(len(labels),1)))\n",
    "    clusters = {}\n",
    "    for i in data:\n",
    "        if i[2] not in clusters:\n",
    "            clusters[i[2]] = [[i[0],i[1]]]\n",
    "        else:\n",
    "            clusters[i[2]].append([i[0],i[1]])\n",
    "    \n",
    "    temp = []\n",
    "    for i in clusters:\n",
    "        tempu = []\n",
    "        for j in clusters[i]:\n",
    "            tempu.append(j[1])\n",
    "        cluster_centroid = np.mean(tempu,axis=0)\n",
    "        if method=='cosine':\n",
    "            temp.append([cosine_similarity([global_centroid,cluster_centroid])[0][1],cluster_centroid,i])\n",
    "        elif method=='euclidean':\n",
    "            temp.append([distance.euclidean(global_centroid,cluster_centroid),cluster_centroid,i])\n",
    "        temp.sort(reverse=True if method=='cosine' else False)\n",
    "\n",
    "    result = []\n",
    "    for i in temp[:25]:\n",
    "        tempu = []\n",
    "        for j in clusters[i[2]]:\n",
    "            if method=='cosine':\n",
    "                tempu.append([cosine_similarity([i[1],j[1]])[0][1],j[0]])\n",
    "            elif method=='euclidean':\n",
    "                tempu.append([distance.euclidean(i[1],j[1]),j[0]])\n",
    "        tempu.sort(reverse=True if method=='cosine' else False)\n",
    "        for j in tempu[:4]:\n",
    "            result.append(j[1])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: Agriculture\n",
      "Unemp_slow\n",
      "Number of articles: 7506 \tNumber of clusters: 55\n",
      "Unemp_avg\n",
      "Number of articles: 5155 \tNumber of clusters: 55\n",
      "Unemp_fast\n",
      "Number of articles: 4884 \tNumber of clusters: 50\n",
      "Agri_slow\n",
      "Number of articles: 14513 \tNumber of clusters: 54\n",
      "Agri_avg\n",
      "Number of articles: 5429 \tNumber of clusters: 60\n",
      "Agri_fast\n",
      "Number of articles: 735 \tNumber of clusters: 60\n",
      "Non_agri_slow\n",
      "Number of articles: 30000 \tNumber of clusters: 54\n",
      "Non_agri_avg\n",
      "Number of articles: 8955 \tNumber of clusters: 59\n",
      "Non_agri_fast\n",
      "Number of articles: 443 \tNumber of clusters: 51\n",
      "Collection: Development\n",
      "Unemp_slow\n",
      "Number of articles: 1426 \tNumber of clusters: 67\n",
      "Unemp_avg\n",
      "Number of articles: 848 \tNumber of clusters: 57\n",
      "Unemp_fast\n",
      "Number of articles: 1034 \tNumber of clusters: 50\n",
      "Agri_slow\n",
      "Number of articles: 2077 \tNumber of clusters: 65\n",
      "Agri_avg\n",
      "Number of articles: 642 \tNumber of clusters: 58\n",
      "Agri_fast\n",
      "Number of articles: 124 \tNumber of clusters: 57\n",
      "Non_agri_slow\n",
      "Number of articles: 11936 \tNumber of clusters: 53\n",
      "Non_agri_avg\n",
      "Number of articles: 1571 \tNumber of clusters: 56\n",
      "Non_agri_fast\n",
      "Number of articles: 137 \tNumber of clusters: 56\n",
      "Collection: Environment\n",
      "Unemp_slow\n",
      "Number of articles: 8720 \tNumber of clusters: 60\n",
      "Unemp_avg\n",
      "Number of articles: 5704 \tNumber of clusters: 60\n",
      "Unemp_fast\n",
      "Number of articles: 4711 \tNumber of clusters: 69\n",
      "Agri_slow\n",
      "Number of articles: 12141 \tNumber of clusters: 71\n",
      "Agri_avg\n",
      "Number of articles: 3885 \tNumber of clusters: 67\n",
      "Agri_fast\n",
      "Number of articles: 650 \tNumber of clusters: 52\n",
      "Non_agri_slow\n",
      "Number of articles: 30000 \tNumber of clusters: 58\n",
      "Non_agri_avg\n",
      "Number of articles: 9444 \tNumber of clusters: 69\n",
      "Non_agri_fast\n",
      "Number of articles: 1261 \tNumber of clusters: 62\n",
      "Collection: Industrialization\n",
      "Unemp_slow\n",
      "Number of articles: 6466 \tNumber of clusters: 66\n",
      "Unemp_avg\n",
      "Number of articles: 4017 \tNumber of clusters: 63\n",
      "Unemp_fast\n",
      "Number of articles: 3898 \tNumber of clusters: 63\n",
      "Agri_slow\n",
      "Number of articles: 8379 \tNumber of clusters: 75\n",
      "Agri_avg\n",
      "Number of articles: 3002 \tNumber of clusters: 51\n",
      "Agri_fast\n",
      "Number of articles: 325 \tNumber of clusters: 50\n",
      "Non_agri_slow\n",
      "Number of articles: 30000 \tNumber of clusters: 61\n",
      "Non_agri_avg\n",
      "Number of articles: 9001 \tNumber of clusters: 62\n",
      "Non_agri_fast\n",
      "Number of articles: 1126 \tNumber of clusters: 61\n",
      "Collection: Lifestyle\n",
      "Unemp_slow\n",
      "Number of articles: 18829 \tNumber of clusters: 64\n",
      "Unemp_avg\n",
      "Number of articles: 12904 \tNumber of clusters: 75\n",
      "Unemp_fast\n",
      "Number of articles: 15173 \tNumber of clusters: 64\n",
      "Agri_slow\n",
      "Number of articles: 19616 \tNumber of clusters: 60\n",
      "Agri_avg\n",
      "Number of articles: 6198 \tNumber of clusters: 58\n",
      "Agri_fast\n",
      "Number of articles: 925 \tNumber of clusters: 52\n",
      "Non_agri_slow\n",
      "Number of articles: 30000 \tNumber of clusters: 54\n",
      "Non_agri_avg\n",
      "Number of articles: 19622 \tNumber of clusters: 68\n",
      "Non_agri_fast\n",
      "Number of articles: 2017 \tNumber of clusters: 50\n"
     ]
    }
   ],
   "source": [
    "datasets = ['dataset_agriculture', 'dataset_development', 'dataset_environment', 'dataset_industrialization', 'dataset_lifestyle']\n",
    "models = ['model_agriculture', 'model_development', 'model_environment', 'model_industrialization', 'model_lifestyle']\n",
    "\n",
    "for dataset, model in zip(datasets,models):\n",
    "\n",
    "    # Printing the collection name.\n",
    "    collection_name = dataset[8:]\n",
    "    print('\\nCollection:',collection_name.capitalize())\n",
    "\n",
    "    # Loading the dataset and the model from the drive.\n",
    "    file = open('./DT2V_Datasets/'+dataset, 'rb')\n",
    "    dataset = pickle.loads(zlib.decompress(pickle.load(file)))\n",
    "    file.close()\n",
    "    model = Doc2Vec.load('./Models/'+model)\n",
    "\n",
    "    # Collecting the article_ids, and corresponding article_vectors for each class.\n",
    "    temp_ids = [set() for _ in range(9)]\n",
    "    temp_vectors = [[] for _ in range(9)]\n",
    "    temp_datasets = [[] for _ in range(9)]\n",
    "    for i in dataset:\n",
    "        if i[6]=='Unemp' and i[7]=='Slow':\n",
    "            if i[0] not in temp_ids[0]:\n",
    "                temp_ids[0].add(i[0])\n",
    "                temp_vectors[0].append(model.docvecs[i[0]])\n",
    "                temp_datasets[0].append([i[0],model.docvecs[i[0]]])\n",
    "        if i[6]=='Unemp' and i[7]=='Average':\n",
    "            if i[0] not in temp_ids[1]:\n",
    "                temp_ids[1].add(i[0])\n",
    "                temp_vectors[1].append(model.docvecs[i[0]])\n",
    "                temp_datasets[1].append([i[0],model.docvecs[i[0]]])\n",
    "        if i[6]=='Unemp' and i[7]=='Fast':\n",
    "            if i[0] not in temp_ids[2]:\n",
    "                temp_ids[2].add(i[0])\n",
    "                temp_vectors[2].append(model.docvecs[i[0]])\n",
    "                temp_datasets[2].append([i[0],model.docvecs[i[0]]])\n",
    "        if i[6]=='Agri' and i[7]=='Slow':\n",
    "            if i[0] not in temp_ids[3]:\n",
    "                temp_ids[3].add(i[0])\n",
    "                temp_vectors[3].append(model.docvecs[i[0]])\n",
    "                temp_datasets[3].append([i[0],model.docvecs[i[0]]])\n",
    "        if i[6]=='Agri' and i[7]=='Average':\n",
    "            if i[0] not in temp_ids[4]:\n",
    "                temp_ids[4].add(i[0])\n",
    "                temp_vectors[4].append(model.docvecs[i[0]])\n",
    "                temp_datasets[4].append([i[0],model.docvecs[i[0]]])\n",
    "        if i[6]=='Agri' and i[7]=='Fast':\n",
    "            if i[0] not in temp_ids[5]:\n",
    "                temp_ids[5].add(i[0])\n",
    "                temp_vectors[5].append(model.docvecs[i[0]])\n",
    "                temp_datasets[5].append([i[0],model.docvecs[i[0]]])\n",
    "        if i[6]=='Non Agri' and i[7]=='Slow':\n",
    "            if i[0] not in temp_ids[6]:\n",
    "                temp_ids[6].add(i[0])\n",
    "                temp_vectors[6].append(model.docvecs[i[0]])\n",
    "                temp_datasets[6].append([i[0],model.docvecs[i[0]]])\n",
    "        if i[6]=='Non Agri' and i[7]=='Average':\n",
    "            if i[0] not in temp_ids[7]:\n",
    "                temp_ids[7].add(i[0])\n",
    "                temp_vectors[7].append(model.docvecs[i[0]])\n",
    "                temp_datasets[7].append([i[0],model.docvecs[i[0]]])\n",
    "        if i[6]=='Non Agri' and i[7]=='Fast':\n",
    "            if i[0] not in temp_ids[8]:\n",
    "                temp_ids[8].add(i[0])\n",
    "                temp_vectors[8].append(model.docvecs[i[0]])\n",
    "                temp_datasets[8].append([i[0],model.docvecs[i[0]]])\n",
    "\n",
    "    # Finding the top titles and keywords for each class.\n",
    "    names = ['unemp_slow','unemp_avg','unemp_fast','agri_slow','agri_avg','agri_fast','non_agri_slow','non_agri_avg','non_agri_fast']\n",
    "    for i in range(9):\n",
    "        print(names[i].capitalize())\n",
    "        result = resolve_articles(hierarchial_clustering(temp_vectors[i],temp_datasets[i]),dataset)\n",
    "        file = open('./Results/'+collection_name+'_'+names[i]+'_'+'titles.csv','w',encoding='utf-8')\n",
    "        for title in result[0]:\n",
    "            file.write(title+'\\n')\n",
    "        file.close()\n",
    "        file = open('./Results/'+collection_name+'_'+names[i]+'_'+'keywords.csv','w')\n",
    "        for keyword in result[1][:100]:\n",
    "            file.write(keyword+'\\n')\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
